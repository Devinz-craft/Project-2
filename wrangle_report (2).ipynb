{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This report is on a project that focuses on the gathering , wrangling and analysis of a dataset related to the WeRateDogs Twitter's data .\n",
    "###### This project is basically divided into three phases ,first being 'Gathering', second being 'Wrangling' and the latter \"Analysis\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### .Gathering\n",
    ">>>> In the first phase (Gathering), i gathered my dataset which would be necessary for my analysis . This project required three (3) dataset .The first was given to me by Udacity and was downloaded manually as a csv file and was named \"twitter_ archive_enhanced\".The second was a tsv file named \"image_prediction\" which was hosted on Udacity server and was downloaded programmatically as a tsv file.The third dataset gotten by scraping the twitter API using python Tweepy's library .\n",
    "\n",
    "\n",
    ">>>N.B:For the third dataset i wasn't given access to a developers account from Twitter as a result i had to use the already scrappred dataset provided by Udacity . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps Explanation:\n",
    ".twitter_archive_file: This data was provided in the projects section . I downloaded the file manually which after I uploaded into my jupyters notebbok space before reading using pandas.\n",
    "\n",
    ".tweet _image_ prediction file: I imported the Python requests and os libraries. With the get() function of the requests library, I got the data through its url and saved it.\n",
    "\n",
    ".tweet_json.txt file: This was suppossed to be done by web scraping but as I mentioned earlier, I wasn't able to get a devolopers account so I opted for the second  option  given by Udacity which was downloading the already scraped file from their server and parsing it into a pandas dataframe ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -Assesing&.Wrangling\n",
    "In the second phase before assessing and cleaning the datasets i made a copy of each,after which I proceeded to assesing each dataset whhere I found errors using both visually and programmatical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Errors Found:\n",
    "This part of the data wrangling process was divided into three parts:\n",
    "\n",
    ".Define:This was to state the error before fixing \n",
    "\n",
    ".Code:This had to do with the fixing stage \n",
    "\n",
    ".Test:This involved checking if the error had been rectified .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dirty issues:\n",
    "1.(df1) In the 'name' column , values that are all lowercase are not dog names.-Visually assessed\n",
    "\n",
    "2.(df1)in_reply_to_status_id column contain a lot of missing values-Programmatically assessed\n",
    "\n",
    "3.(df1)retweeted_status_id column contain a lot of missing values-Programmatically assessed\n",
    "\n",
    "4.(df1) Columns such as 'retweeted_status_id, retweeted_status_user _id, and retweeted_status _timestamp values are that of retweets and won't be used for our analysis- Visually assessed\n",
    "\n",
    "5.(df1 ) Missing values in expanded_urls column- Programmatically assessed\n",
    "\n",
    "6.(df1) Presence of numbers which aren't equals to 10 present in the rating_denominator column-Programmatically assessed\n",
    "\n",
    "7.(df1) in_reply_to_user_id column contain a lot of missing values-Programmatically assessed\n",
    "\n",
    "8.(df1) retweeted_status_timestamp column contain a lot of missing values-Programmatically assessed\n",
    "\n",
    "9.(df1) retweeted_status_user_id column contain a lot of of missing values -Programmatically assessed\n",
    "\n",
    "#### Tidiness issues:Â¶\n",
    "1.(df1)doggo, floofer, pupper, puppo) are categories of dog 'stage' and need to be one column.-Visually assessed\n",
    "\n",
    "2.(df3) Renaming id column to \"tweet_id\" to enable merging-Visually assessed\n",
    "\n",
    "3.(df1) timestamp column is not in datetime data type - Programmatically assessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Storing the Data\n",
    "The cleaned and merged data was saved  in a csv file named twitter_archive_master.csv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Limitation \n",
    "My limitaion was mostly directed to the fact that i couldn't access Twitter's API and wasted a lot of days waiting for a response but apart from that any where I encountered problems I interacted with my instructor who  was a great help  ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
